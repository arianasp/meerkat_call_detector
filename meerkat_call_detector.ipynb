{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlFkdWQOrEci"
   },
   "source": [
    "# **Meerkat Call Detection Framework**\n",
    "\n",
    "This notebook allows you to go through the full process of training a CNN on meerkat calls, running the CNN on new files to extract potential calls, and then evaluating the effectiveness using ROC curves. You can go through all steps or perform a subset of them, but first choose which files you will be using for training and testing.\n",
    "\n",
    "## Important note\n",
    "You may have to authenticate to allow access to Google Drive. Run the code below and it will automatically ask you to authenticate.\n",
    "\n",
    "## Set parameters\n",
    "Here, you set up the parameters needed for the training and testing. Run the cell below before you run the rest of the notebook!\n",
    "\n",
    "**Directories**\n",
    "\n",
    "All directories need to be accessible from your Google Drive.\n",
    "\n",
    "*audio_dir*: This should contain 3 subfolders. 'calls' and 'noise' are folders where the call and noise snippets used for training are located. 'long_recordings' is where full audio files are\n",
    "\n",
    "*groundtruth_dir*: This is the directory containing csv files of labeled data\n",
    "\n",
    "*model_dir*: This directory contains fitted models\n",
    "\n",
    "*output_dir*: This is where the output (.pckl and .csv files) will be stored\n",
    "\n",
    "*code_dir*: Directory where code is stored\n",
    "\n",
    "**Files**\n",
    "\n",
    "*model_name*: file name for the output model, or for the model to load (in the case of using a pre-trained model)\n",
    "\n",
    "**Training parameters**\n",
    "\n",
    "*epochs*: number of epochs to train for\n",
    "\n",
    "*batch_size*: batch size of data to use\n",
    "\n",
    "*steps_per_epoch*: how many training steps per epoch\n",
    "\n",
    "**Prediction parameters**\n",
    "\n",
    "*audio_file*: name of audio file to run prediction on\n",
    "\n",
    "*t_start*: start time for predictions within audio file (sec)\n",
    "\n",
    "*t_end*: end time for predictions within audio file (sec)\n",
    "\n",
    "**Other parameters (probably don't need to change)**\n",
    "\n",
    "*samprate*: sample rate of audio file, should be 8000 for current code (code will likely not work with other sample rates!)\n",
    "\n",
    "*chunk_size*: number of seconds to read in each time wav is accessed directly\n",
    "\n",
    "*chunk_pad*: pad chunks of wavs on each end to avoid any issues - 1 sec is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "9hlUy_XuqhSM",
    "outputId": "1e652e45-3c12-488f-b435-ed45ee088d00",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Loading pretrained model --------\n",
      "Model name: cnn_20epoch__focal_proportional_noaug_2dconv_20190108.h5\n",
      "-------- Done with training or loading model step --------\n",
      "--------Running model on specific file---------\n",
      "/home/arianasp/meerkat_detector/data/full_recordings/HM_VHMM003_SOUNDFOC_20170905_3_label_downsamp.wav\n",
      "-------Running CNN model on new data-------\n",
      "model_path: /home/arianasp/meerkat_detector/models/cnn_20epoch__focal_proportional_noaug_2dconv_20190108.h5\n",
      "wav_path: /home/arianasp/meerkat_detector/data/full_recordings/HM_VHMM003_SOUNDFOC_20170905_3_label_downsamp.wav\n",
      "samprate: 8000\n",
      "t_start: 1\n",
      "t_end: 5044.5\n",
      "\n",
      "--------Generating predictions-------\n",
      "Start time:\n",
      "2019-01-09 09:33:58.345680\n",
      "End time:\n",
      "2019-01-09 09:42:50.725314\n",
      "\n",
      "-------Generating scores and extracting calls-------\n",
      "-------Saving output-------\n",
      "Done\n",
      "/home/arianasp/meerkat_detector/ground_truth/HM_VHMM003_SOUNDFOC_20170905_3_label_downsamp.CSV\n",
      "No ground truth data available for the specified file\n"
     ]
    }
   ],
   "source": [
    "#PARAMETERS - Modify parameters before running to change settings!\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#---------TO CHANGE--------:\n",
    "\n",
    "#General\n",
    "use_pretrained_model = True\n",
    "run_model_on_file = True\n",
    "run_model_on_folder = False\n",
    "run_model_on_specified_round = False\n",
    "specified_round = 2\n",
    "specified_model_name = 'cnn_20epoch__focal_proportional_noaug_2dconv_20190108.h5'  #defaults to None. use only if use_pretrained_model is True or if you want to specify the name of the model directly\n",
    "run_only_where_ground_truth_available = False #set to True to run only on parts of files where groundtruth labels are available\n",
    "evaluate_detections = True\n",
    "focal = True #whether to use focal recordings (or collar data if false)\n",
    "\n",
    "#Model fitting options (use only if use_pretrained_model is False)\n",
    "epochs = 20 #Number of epochs to train for\n",
    "augment = False #whether to augment by overlaying noise (at different levels) on calls\n",
    "conv_dimension = 2\n",
    "\n",
    "#name of audio file (use only if run_model_on_file is True)\n",
    "#audio_file_to_predict = 'HM_PET_R11_20170903-20170908_file_2_(2017_09_03-05_44_59)_ASWMUX221163_SS.wav'\n",
    "audio_file_to_predict = 'HM_VHMM003_SOUNDFOC_20170905_3_label_downsamp.wav'\n",
    "\n",
    "#Name of audio folder for prediction (use only if run_model_on_folder is True)\n",
    "audio_folder = None\n",
    "\n",
    "#Probability of selecting each call type for training (call types given below). If None, choose calls with probability equal to their occurrence in the training data\n",
    "call_probs = None\n",
    "\n",
    "#---------TO LEAVE ALONE (PROBABLY)--------:\n",
    "\n",
    "#Main directory\n",
    "base_dir = '/home/arianasp/meerkat_detector' #base directory\n",
    "\n",
    "#Subdirectories\n",
    "audio_dir = base_dir + '/data/full_recordings'\n",
    "ground_truth_dir = base_dir + '/ground_truth'\n",
    "model_dir = base_dir + '/models'\n",
    "output_dir = base_dir + '/predictions'\n",
    "code_dir = base_dir + '/dev'\n",
    "if(focal):\n",
    "    clips_dir = base_dir + '/clips_foc'\n",
    "else:\n",
    "    clips_dir = base_dir + '/clips'\n",
    "eval_dir = base_dir + '/eval'\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 100\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "call_types = ['cc','sn','ld','mov','agg','alarm','soc','hyb','unk','oth']\n",
    "\n",
    "#Evaluation parameters\n",
    "boundary_thresh = 0.6\n",
    "n_points = 30\n",
    "pckl_paths = [] #NOTE: One can specify pckl_paths, but this is only useful when running evluation ALONE, otherwise anything specified here gets cleared\n",
    "\n",
    "#Other parameters\n",
    "samprate = 8000 \n",
    "chunk_size = 60 \n",
    "chunk_pad = 1 \n",
    "mel = False\n",
    "ml_plan_file = base_dir + '/docs/' + 'audio_labeling_plan_filenames.csv'\n",
    "\n",
    "#SETUP\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#import libraries\n",
    "import sys\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import glob\n",
    "import audioread\n",
    "\n",
    "#Set path\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "#Import call detector library\n",
    "from meerkat_call_detector_library import *\n",
    "\n",
    "#NEW MODEL CONSTRUCTION DEFINITIONS (TO MOVE LATER)\n",
    "#inputs: originally spectrogram or output of upper layer\n",
    "#filters: number of filters to use (arbitrary)\n",
    "#n_convs: number of consecutive convolutions to do\n",
    "#output will be time_dim(inputs)/2 x n_filters\n",
    "\n",
    "\n",
    "#Set up file names and paths\n",
    "aug_str = 'noaug'\n",
    "if(augment):\n",
    "    aug_str = 'aug'\n",
    "    \n",
    "dim_str = '_1dconv_'\n",
    "if(conv_dimension==2):\n",
    "    dim_str = '_2dconv_'\n",
    "    \n",
    "focal_str = '_collar_'\n",
    "if(focal):\n",
    "    focal_str = '_focal_'\n",
    "\n",
    "if(call_probs is not None):\n",
    "    call_probs_str = ''.join([str(s) + '_' for s in call_probs])\n",
    "else:\n",
    "    call_probs_str = 'proportional_'\n",
    "\n",
    "#if model name was specified, use specified name here. otherwise construct name based on parameters.\n",
    "if(specified_model_name is not None):\n",
    "    model_name = specified_model_name\n",
    "else:\n",
    "    model_name = 'cnn_' + str(epochs) + 'epoch_' +  focal_str + call_probs_str + aug_str + dim_str + time.strftime('%Y%m%d') + '.h5'\n",
    "model_path = model_dir + '/' + model_name\n",
    "\n",
    "#create threshold range for evaluation\n",
    "thresh_range = np.linspace(boundary_thresh+.0001,.9,10)\n",
    "\n",
    "for i in range(2,n_points-9):\n",
    "    thresh_range = np.append(thresh_range,thresh_range[len(thresh_range)-1]+10**(-i)*9)\n",
    "    \n",
    "#TRAIN OR LOAD MODEL\n",
    "#-------------------------------------------------------------------------------\n",
    "        \n",
    "if(use_pretrained_model):\n",
    "\n",
    "    print(\"-------- Loading pretrained model --------\")\n",
    "    print('Model name: ' + model_name)\n",
    "  \n",
    "    #Load pre-trained model\n",
    "    model = load_model(model_dir + '/' + model_name)\n",
    "\n",
    "else:\n",
    "  \n",
    "    print(\"-------- Training new model --------\")\n",
    "    print('Start time:')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    #Construct model\n",
    "    if(conv_dimension==1):\n",
    "        model = construct_unet_model()\n",
    "    else:\n",
    "        model = construct_unet_model_2d()\n",
    "    model.summary()\n",
    "\n",
    "    #Fit model\n",
    "    model.fit_generator(data_generator(clips_dir = clips_dir,batch_size = batch_size, cnn_dim=conv_dimension,mel=mel), epochs=epochs, use_multiprocessing=True, workers=16, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    print('End time:')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    #Save fitted model\n",
    "    print('Saving model as: ' + model_name)\n",
    "    model.save(filepath=model_dir + '/' + model_name) \n",
    "\n",
    "print(\"-------- Done with training or loading model step --------\")\n",
    "\n",
    "\n",
    "#RUN MODEL TO DETECT CALLS\n",
    "#-------------------------------------------------------------------------------\n",
    "#Extract probable calls from wav recording\n",
    "                    \n",
    "if(run_model_on_folder):\n",
    "    \n",
    "    print(\"-------- Running model on folder --------\")\n",
    "    print('Folder path = ' + audio_folder)\n",
    "    \n",
    "    #get all audio files in that folder (or subfolders of it, recursively)\n",
    "    audio_files = glob.glob(audio_folder + '/**/' + '*.wav',recursive=True)\n",
    "    \n",
    "    #print number of files found\n",
    "    print('Found ' + str(len(audio_files)) + ' audio files, running model on all of them')\n",
    "    \n",
    "if(run_model_on_specified_round):\n",
    "    \n",
    "    print(\"-------- Running model on specified round --------\")\n",
    "    print('Round = ' + str(specified_round))\n",
    "\n",
    "    ml_plan = pandas.read_csv(ml_plan_file)\n",
    "    files_to_run = ml_plan[(ml_plan['Round (0-1)'] == str(specified_round)) | (ml_plan['Round (1-2)'] == str(specified_round)) | (ml_plan['Round (2+)'] == str(specified_round))]\n",
    "    files_to_run = files_to_run['Audio filename'].tolist()\n",
    "\n",
    "    audio_files = list()\n",
    "\n",
    "    for f_idx in range(len(files_to_run)):\n",
    "        curr_file = glob.glob(base_dir + '/data/raw_data' + '/**/' + files_to_run[f_idx], recursive=True)[0]\n",
    "        audio_files.append(curr_file)\n",
    "\n",
    "    #print number of files found\n",
    "    print('Found ' + str(len(audio_files)) + ' audio files, running model on all of them')\n",
    "\n",
    "if(run_model_on_folder or run_model_on_specified_round):\n",
    "    \n",
    "    for i in range(len(audio_files)):\n",
    "        \n",
    "        audio_file = audio_files[i]\n",
    "        \n",
    "        print('Running predictions on file: ')\n",
    "        print(audio_file)\n",
    "        \n",
    "        aud = wave.open(audio_file,'rb')\n",
    "        \n",
    "        #time bounds for extraction\n",
    "        if(run_only_where_ground_truth_available):\n",
    "            labels = get_ground_truth_labels(wav_name = os.path.basename(audio_file), ground_truth_dir = ground_truth_dir)\n",
    "            if(labels is None):\n",
    "                print('No ground truth data found - skipping this file')\n",
    "                continue\n",
    "            else:\n",
    "                [t_start, t_end] = get_start_end_time_labels(labels)\n",
    "        else:\n",
    "            t_start = 1\n",
    "            t_end = np.floor(aud.getnframes()/aud.getframerate()/60)*60\n",
    "        \n",
    "        #start at least 1 sec in to avoid problems of wrong input size in next step\n",
    "        if(t_start < 1):\n",
    "            t_start = 1\n",
    "        \n",
    "        #Store parameters in extraction_params object\n",
    "        wav_path = audio_file\n",
    "        audio_name = os.path.basename(audio_file)\n",
    "        pckl_path = output_dir + '/' + audio_name[0:(len(audio_name)-4)] + \"_label_\" + model_name[0:(len(model_name)-3)] + '_' + str(t_start) + '-' + str(t_end) + \".pckl\"\n",
    "        \n",
    "        #Append to list of created pckl paths\n",
    "        pckl_paths.append(pckl_path)\n",
    "        \n",
    "        #if path to extraction results already exists, do not run. otherwise run.\n",
    "        if(not(os.path.exists(pckl_path))):\n",
    "            extraction_params = CallExtractionParams(model_path = model_path, wav_path = wav_path, pckl_path=pckl_path, samprate = samprate, t_start = t_start, t_end = t_end)\n",
    "            \n",
    "            #if SOUNDFOC is in filename, this indicates a different type of sound file - don't run!\n",
    "            if(re.search('SOUNDFOC',audio_file)==None):\n",
    "                extract_scores(model, extraction_params, mel=mel)\n",
    "\n",
    "#Run model on a specific file\n",
    "if(run_model_on_file):\n",
    "    \n",
    "    print('--------Running model on specific file---------')\n",
    "\n",
    "    #create paths to prediction files (wav and pckl)\n",
    "    wav_path = audio_dir + '/' + audio_file_to_predict\n",
    "    \n",
    "    print(wav_path)\n",
    "    \n",
    "    #tibase_dir = '/home/arianasp/meerkat_detector'me bounds for extraction\n",
    "    if(run_only_where_ground_truth_available): #TODO: add option to find labeled portion and run only for this\n",
    "        labels = get_ground_truth_labels(wav_name = audio_file_to_predict, ground_truth_dir = ground_truth_dir)\n",
    "        if(labels is None):\n",
    "            print('No ground truth data found - set run_only_where_ground_truth_available to False to run on this file')\n",
    "            t_start = None\n",
    "        else:\n",
    "            [t_start, t_end] = get_start_end_time_labels(labels)\n",
    "        #start at least 1 sec in to avoid problems with wrong matrix size in next step\n",
    "        if t_start < 1:\n",
    "            t_start = 1\n",
    "    else:\n",
    "        t_start = 1\n",
    "        with audioread.audio_open(wav_path) as f:\n",
    "            t_end = f.duration - 1\n",
    "        \n",
    "    if(t_start is not None):\n",
    "    \n",
    "        #create path to output file\n",
    "        pckl_path = output_dir + '/' + audio_file_to_predict[0:(len(audio_file_to_predict)-4)] + \"_label_\" + model_name[0:(len(model_name)-3)] + '_' + str(t_start) + '-' + str(t_end) + \".pckl\"\n",
    "    \n",
    "        pckl_paths = [pckl_path]\n",
    "        extraction_params = CallExtractionParams(model_path = model_path, wav_path = wav_path, pckl_path = pckl_path, samprate = samprate, t_start = t_start, t_end = t_end)\n",
    "        extract_scores(model, extraction_params, mel=mel)\n",
    "                \n",
    "#EVALUATE DETECTIONS\n",
    "#-------------------------------------------------------------------------------\n",
    "if(evaluate_detections & (pckl_paths is not None)):\n",
    "    \n",
    "    #for file_idx in range(len(pckl_files)):\n",
    "    for file_idx in range(len(pckl_paths)):\n",
    "\n",
    "        pckl_path = pckl_paths[file_idx]\n",
    "\n",
    "        run_evaluation(pckl_path = pckl_path,thresh_range=thresh_range,save_dir =eval_dir,ground_truth_dir = ground_truth_dir,call_types = call_types, verbose = False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "meerkat_call_detector.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
