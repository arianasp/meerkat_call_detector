{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlFkdWQOrEci"
   },
   "source": [
    "# **Meerkat Call Detection Framework**\n",
    "\n",
    "This notebook allows you to go through the full process of training a CNN on meerkat calls, running the CNN on new files to extract potential calls, and then evaluating the effectiveness using ROC curves. You can go through all steps or perform a subset of them, but first choose which files you will be using for training and testing.\n",
    "\n",
    "## Important note\n",
    "You may have to authenticate to allow access to Google Drive. Run the code below and it will automatically ask you to authenticate.\n",
    "\n",
    "## Set parameters\n",
    "Here, you set up the parameters needed for the training and testing. Run the cell below before you run the rest of the notebook!\n",
    "\n",
    "**Directories**\n",
    "\n",
    "All directories need to be accessible from your Google Drive.\n",
    "\n",
    "*audio_dir*: This should contain 3 subfolders. 'calls' and 'noise' are folders where the call and noise snippets used for training are located. 'long_recordings' is where full audio files are\n",
    "\n",
    "*groundtruth_dir*: This is the directory containing csv files of labeled data\n",
    "\n",
    "*model_dir*: This directory contains fitted models\n",
    "\n",
    "*output_dir*: This is where the output (.pckl and .csv files) will be stored\n",
    "\n",
    "*code_dir*: Directory where code is stored\n",
    "\n",
    "**Files**\n",
    "\n",
    "*model_name*: file name for the output model, or for the model to load (in the case of using a pre-trained model)\n",
    "\n",
    "**Training parameters**\n",
    "\n",
    "*epochs*: number of epochs to train for\n",
    "\n",
    "*batch_size*: batch size of data to use\n",
    "\n",
    "*steps_per_epoch*: how many training steps per epoch\n",
    "\n",
    "**Prediction parameters**\n",
    "\n",
    "*audio_file*: name of audio file to run prediction on\n",
    "\n",
    "*t_start*: start time for predictions within audio file (sec)\n",
    "\n",
    "*t_end*: end time for predictions within audio file (sec)\n",
    "\n",
    "**Other parameters (probably don't need to change)**\n",
    "\n",
    "*samprate*: sample rate of audio file, should be 8000 for current code (code will likely not work with other sample rates!)\n",
    "\n",
    "*chunk_size*: number of seconds to read in each time wav is accessed directly\n",
    "\n",
    "*chunk_pad*: pad chunks of wavs on each end to avoid any issues - 1 sec is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "9hlUy_XuqhSM",
    "outputId": "1e652e45-3c12-488f-b435-ed45ee088d00",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Training model --------\n",
      "Model name: cnn_1epoch__collar__orig_proportional_aug_2dconv_20190219.h5\n",
      "Start time:\n",
      "2019-02-19 15:41:14.047240\n",
      "Creating new model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 512, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 512, 128, 32) 320         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 512, 128, 32) 0           conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 512, 128, 32) 9248        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 512, 128, 32) 0           conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 512, 128, 32) 9248        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 512, 128, 32) 0           conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_25 (AveragePo (None, 256, 64, 32)  0           activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 256, 64, 32)  9248        average_pooling2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 256, 64, 32)  0           conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 256, 64, 32)  9248        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 256, 64, 32)  0           conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 256, 64, 32)  9248        activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 256, 64, 32)  0           conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_26 (AveragePo (None, 128, 32, 32)  0           activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 128, 32, 64)  18496       average_pooling2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 128, 32, 64)  0           conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 128, 32, 64)  36928       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 128, 32, 64)  0           conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 128, 32, 64)  36928       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 128, 32, 64)  0           conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_27 (AveragePo (None, 64, 16, 64)   0           activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 64, 16, 96)   55392       average_pooling2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 64, 16, 96)   0           conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 64, 16, 96)   83040       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 64, 16, 96)   0           conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 64, 16, 96)   83040       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 64, 16, 96)   0           conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_28 (AveragePo (None, 32, 8, 96)    0           activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 32, 8, 128)   110720      average_pooling2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 32, 8, 128)   0           conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 32, 8, 128)   147584      activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 32, 8, 128)   0           conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 32, 8, 128)   147584      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 32, 8, 128)   0           conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_29 (AveragePo (None, 16, 4, 128)   0           activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 16, 4, 160)   184480      average_pooling2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 16, 4, 160)   0           conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 16, 4, 160)   230560      activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 16, 4, 160)   0           conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 4, 160)   230560      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 16, 4, 160)   0           conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_30 (AveragePo (None, 8, 2, 160)    0           activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 8, 2, 160)    230560      average_pooling2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 8, 2, 160)    25760       average_pooling2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 8, 2, 160)    0           conv2d_143[0][0]                 \n",
      "                                                                 conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 8, 2, 160)    0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_25 (UpSampling2D) (None, 16, 4, 160)   0           activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 4, 128)   184448      up_sampling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 4, 128)   16512       average_pooling2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 16, 4, 128)   0           conv2d_145[0][0]                 \n",
      "                                                                 conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 16, 4, 128)   0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_26 (UpSampling2D) (None, 32, 8, 128)   0           activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 32, 8, 96)    110688      up_sampling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 32, 8, 96)    9312        average_pooling2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 32, 8, 96)    0           conv2d_147[0][0]                 \n",
      "                                                                 conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 32, 8, 96)    0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_27 (UpSampling2D) (None, 64, 16, 96)   0           activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 64, 16, 64)   55360       up_sampling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 64, 16, 64)   4160        average_pooling2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 64, 16, 64)   0           conv2d_149[0][0]                 \n",
      "                                                                 conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 64, 16, 64)   0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_28 (UpSampling2D) (None, 128, 32, 64)  0           activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 128, 32, 32)  18464       up_sampling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 128, 32, 32)  1056        average_pooling2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 128, 32, 32)  0           conv2d_151[0][0]                 \n",
      "                                                                 conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 128, 32, 32)  0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_29 (UpSampling2D) (None, 256, 64, 32)  0           activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 256, 64, 32)  9248        up_sampling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 256, 64, 32)  1056        average_pooling2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 256, 64, 32)  0           conv2d_153[0][0]                 \n",
      "                                                                 conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 256, 64, 32)  0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_30 (UpSampling2D) (None, 512, 128, 32) 0           activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 512, 128, 1)  33          up_sampling2d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 512, 128)     0           conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 512, 1)       129         reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 512, 1)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 512, 1, 1)    0           activation_125[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,078,658\n",
      "Trainable params: 2,078,658\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arianasp/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 96s 961ms/step - loss: 0.4116\n",
      "End time:\n",
      "2019-02-19 15:42:51.191129\n",
      "Saving model as: cnn_1epoch__collar__orig_proportional_aug_2dconv_20190219.h5\n",
      "-------- Done with training or loading model step --------\n",
      "--------Running model on specific file---------\n",
      "/home/arianasp/meerkat_detector/data/full_recordings/HM_PET_R11_20170903-20170908_file_2_(2017_09_03-05_44_59)_ASWMUX221163_SS.wav\n",
      "1\n",
      "16250.6085\n",
      "-------Running CNN model on new data-------\n",
      "model_path: /home/arianasp/meerkat_detector/models/cnn_1epoch__collar__orig_proportional_aug_2dconv_20190219.h5\n",
      "wav_path: /home/arianasp/meerkat_detector/data/full_recordings/HM_PET_R11_20170903-20170908_file_2_(2017_09_03-05_44_59)_ASWMUX221163_SS.wav\n",
      "samprate: 8000\n",
      "t_start: 1\n",
      "t_end: 16250.6085\n",
      "\n",
      "--------Generating predictions-------\n",
      "Start time:\n",
      "2019-02-19 15:42:58.145576\n"
     ]
    }
   ],
   "source": [
    "#PARAMETERS - Modify parameters before running to change settings!\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#---------TO CHANGE--------:\n",
    "\n",
    "#General\n",
    "use_pretrained_model = False #whether to load a pre-trained model (either to go to evaluation or to continue training with new data)\n",
    "train_model = True #whether or not to train model or skip straight to evaluation\n",
    "run_model_on_file = True\n",
    "run_model_on_folder = False\n",
    "run_model_on_specified_round = False\n",
    "specified_round = 2\n",
    "specified_model_name = None #defaults to None. use only if use_pretrained_model is True or if you want to specify the name of the model directly\n",
    "run_only_where_ground_truth_available = False #set to True to run only on parts of files where groundtruth labels are available\n",
    "evaluate_detections = True\n",
    "focal = False #whether to use focal recordings (or collar data if false)\n",
    "focal_megan = False\n",
    "\n",
    "foc_megan_dir = '/media/arianasp/Elements/Sound Files/Pups VS No Pups/Hakuna Matata/NO PUPS'\n",
    "\n",
    "#Model fitting options (use only if train_model is True)\n",
    "epochs = 1 #Number of epochs to train for\n",
    "augment = True #whether to augment by overlaying noise (at different levels) on calls\n",
    "conv_dimension = 2\n",
    "\n",
    "#name of audio file (use only if run_model_on_file is True)\n",
    "audio_file_to_predict = 'HM_PET_R11_20170903-20170908_file_2_(2017_09_03-05_44_59)_ASWMUX221163_SS.wav'\n",
    "#audio_file_to_predict = 'VCVM001_HM_1_17JAN2017_LEFT_8000.wav'\n",
    "\n",
    "#Name of audio folder for prediction (use only if run_model_on_folder is True)\n",
    "audio_folder = None\n",
    "\n",
    "#Probability of selecting each call type for training (call types given below). If None, choose calls with probability equal to their occurrence in the training data\n",
    "call_probs = None\n",
    "\n",
    "#---------TO LEAVE ALONE (PROBABLY)--------:\n",
    "\n",
    "#Main directory\n",
    "base_dir = '/home/arianasp/meerkat_detector' #base directory\n",
    "\n",
    "#Subdirectories\n",
    "audio_dir = base_dir + '/data/full_recordings'\n",
    "if focal_megan:\n",
    "    audio_dir = foc_megan_dir\n",
    "\n",
    "ground_truth_dir = base_dir + '/ground_truth'\n",
    "model_dir = base_dir + '/models'\n",
    "output_dir = base_dir + '/predictions'\n",
    "code_dir = base_dir + '/dev'\n",
    "if(focal):\n",
    "    clips_dir = base_dir + '/clips_foc'\n",
    "elif(focal_megan):\n",
    "    clips_dir = base_dir +'/clips_foc_megan'\n",
    "else:\n",
    "    clips_dir = base_dir + '/clips'\n",
    "eval_dir = base_dir + '/eval'\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 100\n",
    "steps_per_epoch = 1000\n",
    "steps_per_epoch = 100\n",
    "call_types = ['cc','sn','ld','mov','agg','alarm','soc','hyb','unk','oth']\n",
    "\n",
    "#Evaluation parameters\n",
    "boundary_thresh = 0.6\n",
    "n_points = 30\n",
    "pckl_paths = ['/home/arianasp/meerkat_detector/predictions/HM_PET_R11_20170903-20170908_file_2_(2017_09_03-05_44_59)_ASWMUX221163_SS_label_cnn_5epoch__collar__orig_proportional_aug_2dconv_20190219_1-16250.6085.pckl'] #NOTE: One can specify pckl_paths, but this is only useful when running evluation ALONE, otherwise anything specified here gets cleared\n",
    "\n",
    "#Other parameters\n",
    "samprate = 8000 \n",
    "chunk_size = 60 \n",
    "chunk_pad = 1 \n",
    "ml_plan_file = base_dir + '/docs/' + 'audio_labeling_plan_filenames.csv'\n",
    "\n",
    "#SETUP\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#import libraries\n",
    "import sys\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import glob\n",
    "import audioread\n",
    "\n",
    "#Set path\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "#Import call detector library\n",
    "from meerkat_call_detector_library import *\n",
    "\n",
    "#NEW MODEL CONSTRUCTION DEFINITIONS (TO MOVE LATER)\n",
    "#inputs: originally spectrogram or output of upper layer\n",
    "#filters: number of filters to use (arbitrary)\n",
    "#n_convs: number of consecutive convolutions to do\n",
    "#output will be time_dim(inputs)/2 x n_filters\n",
    "\n",
    "\n",
    "#Set up file names and paths\n",
    "aug_str = 'noaug'\n",
    "if(augment):\n",
    "    aug_str = 'aug'\n",
    "    \n",
    "dim_str = '_1dconv_'\n",
    "if(conv_dimension==2):\n",
    "    dim_str = '_2dconv_'\n",
    "    \n",
    "focal_str = '_collar_'\n",
    "if(focal):\n",
    "    focal_str = '_focal_'\n",
    "elif(focal_megan):\n",
    "    focal_str = '_focalmeg_'\n",
    "\n",
    "if(call_probs is not None):\n",
    "    call_probs_str = ''.join([str(s) + '_' for s in call_probs])\n",
    "else:\n",
    "    call_probs_str = 'proportional_'\n",
    "    \n",
    "pretrain_str = '_orig_'\n",
    "if(use_pretrained_model):\n",
    "    pretrain_str = '_pretrained_'\n",
    "\n",
    "#if model name was specified, use specified name here. otherwise construct name based on parameters.\n",
    "if(not(train_model)):\n",
    "    model_name = specified_model_name\n",
    "else:\n",
    "    model_name = 'cnn_' + str(epochs) + 'epoch_' +  focal_str + pretrain_str + call_probs_str + aug_str + dim_str + time.strftime('%Y%m%d') + '.h5'\n",
    "model_path = model_dir + '/' + model_name\n",
    "\n",
    "#create threshold range for evaluation\n",
    "thresh_range = np.linspace(boundary_thresh+.0001,.9,10)\n",
    "\n",
    "for i in range(2,n_points-9):\n",
    "    thresh_range = np.append(thresh_range,thresh_range[len(thresh_range)-1]+10**(-i)*9)\n",
    "    \n",
    "#TRAIN OR LOAD MODEL\n",
    "#-------------------------------------------------------------------------------\n",
    "        \n",
    "if(use_pretrained_model):\n",
    "\n",
    "    print(\"-------- Loading pretrained model --------\")\n",
    "    print('Model name: ' + specified_model_name)\n",
    "  \n",
    "    #Load pre-trained model\n",
    "    model = load_model(model_dir + '/' + specified_model_name)\n",
    "\n",
    "if(train_model):\n",
    "  \n",
    "    print(\"-------- Training model --------\")\n",
    "    print('Model name: ' + model_name)\n",
    "    print('Start time:')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    if(not(use_pretrained_model)):\n",
    "        print('Creating new model')\n",
    "        #Construct model\n",
    "        if(conv_dimension==1):\n",
    "            model = construct_unet_model()\n",
    "        else:\n",
    "            model = construct_unet_model_2d()\n",
    "        model.summary()\n",
    "\n",
    "    #Fit model\n",
    "    model.fit_generator(data_generator(clips_dir = clips_dir,batch_size = batch_size, cnn_dim=conv_dimension), epochs=epochs, use_multiprocessing=True, workers=16, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    print('End time:')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    #Save fitted model\n",
    "    print('Saving model as: ' + model_name)\n",
    "    model.save(filepath=model_dir + '/' + model_name) \n",
    "\n",
    "print(\"-------- Done with training or loading model step --------\")\n",
    "\n",
    "\n",
    "#RUN MODEL TO DETECT CALLS\n",
    "#-------------------------------------------------------------------------------\n",
    "#Extract probable calls from wav recording\n",
    "                    \n",
    "if(run_model_on_folder):\n",
    "    \n",
    "    print(\"-------- Running model on folder --------\")\n",
    "    print('Folder path = ' + audio_folder)\n",
    "    \n",
    "    #get all audio files in that folder (or subfolders of it, recursively)\n",
    "    audio_files = glob.glob(audio_folder + '/**/' + '*.wav',recursive=True)\n",
    "    \n",
    "    #print number of files found\n",
    "    print('Found ' + str(len(audio_files)) + ' audio files, running model on all of them')\n",
    "    \n",
    "if(run_model_on_specified_round):\n",
    "    \n",
    "    print(\"-------- Running model on specified round --------\")\n",
    "    print('Round = ' + str(specified_round))\n",
    "\n",
    "    ml_plan = pandas.read_csv(ml_plan_file)\n",
    "    files_to_run = ml_plan[(ml_plan['Round (0-1)'] == str(specified_round)) | (ml_plan['Round (1-2)'] == str(specified_round)) | (ml_plan['Round (2+)'] == str(specified_round))]\n",
    "    files_to_run = files_to_run['Audio filename'].tolist()\n",
    "\n",
    "    audio_files = list()\n",
    "\n",
    "    for f_idx in range(len(files_to_run)):\n",
    "        curr_file = glob.glob(base_dir + '/data/raw_data' + '/**/' + files_to_run[f_idx], recursive=True)[0]\n",
    "        audio_files.append(curr_file)\n",
    "\n",
    "    #print number of files found\n",
    "    print('Found ' + str(len(audio_files)) + ' audio files, running model on all of them')\n",
    "\n",
    "if(run_model_on_folder or run_model_on_specified_round):\n",
    "    \n",
    "    for i in range(len(audio_files)):\n",
    "        \n",
    "        audio_file = audio_files[i]\n",
    "        \n",
    "        print('Running predictions on file: ')\n",
    "        print(audio_file)\n",
    "        \n",
    "        aud = wave.open(audio_file,'rb')\n",
    "        \n",
    "        #time bounds for extraction\n",
    "        if(run_only_where_ground_truth_available):\n",
    "            labels = get_ground_truth_labels(wav_name = os.path.basename(audio_file), ground_truth_dir = ground_truth_dir)\n",
    "            if(labels is None):\n",
    "                print('No ground truth data found - skipping this file')\n",
    "                continue\n",
    "            else:\n",
    "                [t_start, t_end] = get_start_end_time_labels(labels)\n",
    "        else:\n",
    "            t_start = 1\n",
    "            t_end = np.floor(aud.getnframes()/aud.getframerate()/60)*60-1\n",
    "        \n",
    "        #start at least 1 sec in to avoid problems of wrong input size in next step\n",
    "        if(t_start < 1):\n",
    "            t_start = 1\n",
    "        \n",
    "        #Store parameters in extraction_params object\n",
    "        wav_path = audio_file\n",
    "        audio_name = os.path.basename(audio_file)\n",
    "        pckl_path = output_dir + '/' + audio_name[0:(len(audio_name)-4)] + \"_label_\" + model_name[0:(len(model_name)-3)] + '_' + str(t_start) + '-' + str(t_end) + \".pckl\"\n",
    "        \n",
    "        #Append to list of created pckl paths\n",
    "        pckl_paths.append(pckl_path)\n",
    "        \n",
    "        #if path to extraction results already exists, do not run. otherwise run.\n",
    "        if(not(os.path.exists(pckl_path))):\n",
    "            extraction_params = CallExtractionParams(model_path = model_path, wav_path = wav_path, pckl_path=pckl_path, samprate = samprate, t_start = t_start, t_end = t_end)\n",
    "            \n",
    "            #if SOUNDFOC is in filename, this indicates a different type of sound file - don't run!\n",
    "            if(re.search('SOUNDFOC',audio_file)==None):\n",
    "                extract_scores(model, extraction_params)\n",
    "\n",
    "#Run model on a specific file\n",
    "if(run_model_on_file):\n",
    "    \n",
    "    print('--------Running model on specific file---------')\n",
    "\n",
    "    #create paths to prediction files (wav and pckl)\n",
    "    wav_path = audio_dir + '/' + audio_file_to_predict\n",
    "    \n",
    "    print(wav_path)\n",
    "    \n",
    "    #tibase_dir = '/home/arianasp/meerkat_detector'me bounds for extraction\n",
    "    if(run_only_where_ground_truth_available): #TODO: add option to find labeled portion and run only for this\n",
    "        labels = get_ground_truth_labels(wav_name = audio_file_to_predict, ground_truth_dir = ground_truth_dir)\n",
    "        if(labels is None):\n",
    "            print('No ground truth data found - set run_only_where_ground_truth_available to False to run on this file')\n",
    "            t_start = None\n",
    "        else:\n",
    "            [t_start, t_end] = get_start_end_time_labels(labels)\n",
    "            t_end = t_end - 1\n",
    "        #start at least 1 sec in to avoid problems with wrong matrix size in next step\n",
    "        if t_start < 1:\n",
    "            t_start = 1\n",
    "    else:\n",
    "        t_start = 1\n",
    "        with audioread.audio_open(wav_path) as f:\n",
    "            t_end = f.duration - 1.5\n",
    "        \n",
    "    if(t_start is not None):\n",
    "    \n",
    "        #create path to output file\n",
    "        pckl_path = output_dir + '/' + audio_file_to_predict[0:(len(audio_file_to_predict)-4)] + \"_label_\" + model_name[0:(len(model_name)-3)] + '_' + str(t_start) + '-' + str(t_end) + \".pckl\"\n",
    "    \n",
    "        pckl_paths = [pckl_path]\n",
    "        extraction_params = CallExtractionParams(model_path = model_path, wav_path = wav_path, pckl_path = pckl_path, samprate = samprate, t_start = t_start, t_end = t_end)\n",
    "        print(extraction_params.t_start)\n",
    "        print(extraction_params.t_end)\n",
    "        extract_scores(model, extraction_params)\n",
    "    \n",
    "#EVALUATE DETECTIONS\n",
    "#-------------------------------------------------------------------------------\n",
    "if(evaluate_detections & (pckl_paths is not None)):\n",
    "    \n",
    "    #for file_idx in range(len(pckl_files)):\n",
    "    for file_idx in range(len(pckl_paths)):\n",
    "\n",
    "        pckl_path = pckl_paths[file_idx]\n",
    "\n",
    "        run_evaluation(pckl_path = pckl_path,thresh_range=thresh_range,save_dir =eval_dir,ground_truth_dir = ground_truth_dir,call_types = call_types, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5045.52\n",
      "/home/arianasp/meerkat_detector/data/full_recordings\n",
      "5044.5\n"
     ]
    }
   ],
   "source": [
    "print(extraction_params.t_start)\n",
    "print(extraction_params.t_end)\n",
    "print(audio_dir)\n",
    "with audioread.audio_open(wav_path) as f:\n",
    "    t_end = f.duration - 1\n",
    "print(t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x7fcecc765cc0>\n",
      "/home/arianasp/meerkat_detector/clips_foc_megan\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "extraction_params.t_start\n",
    "print(clips_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "meerkat_call_detector.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
