{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlFkdWQOrEci"
   },
   "source": [
    "# **Meerkat Call Detection Framework**\n",
    "\n",
    "This notebook allows you to go through the full process of training a CNN on meerkat calls, running the CNN on new files to extract potential calls, and then evaluating the effectiveness using ROC curves. You can go through all steps or perform a subset of them, but first choose which files you will be using for training and testing.\n",
    "\n",
    "## Important note\n",
    "You may have to authenticate to allow access to Google Drive. Run the code below and it will automatically ask you to authenticate.\n",
    "\n",
    "## Set parameters\n",
    "Here, you set up the parameters needed for the training and testing. Run the cell below before you run the rest of the notebook!\n",
    "\n",
    "**Directories**\n",
    "\n",
    "All directories need to be accessible from your Google Drive.\n",
    "\n",
    "*audio_dir*: This should contain 3 subfolders. 'calls' and 'noise' are folders where the call and noise snippets used for training are located. 'long_recordings' is where full audio files are\n",
    "\n",
    "*groundtruth_dir*: This is the directory containing csv files of labeled data\n",
    "\n",
    "*model_dir*: This directory contains fitted models\n",
    "\n",
    "*output_dir*: This is where the output (.pckl and .csv files) will be stored\n",
    "\n",
    "*code_dir*: Directory where code is stored\n",
    "\n",
    "**Files**\n",
    "\n",
    "*model_name*: file name for the output model, or for the model to load (in the case of using a pre-trained model)\n",
    "\n",
    "**Training parameters**\n",
    "\n",
    "*epochs*: number of epochs to train for\n",
    "\n",
    "*batch_size*: batch size of data to use\n",
    "\n",
    "*steps_per_epoch*: how many training steps per epoch\n",
    "\n",
    "**Prediction parameters**\n",
    "\n",
    "*audio_file*: name of audio file to run prediction on\n",
    "\n",
    "*t_start*: start time for predictions within audio file (sec)\n",
    "\n",
    "*t_end*: end time for predictions within audio file (sec)\n",
    "\n",
    "**Other parameters (probably don't need to change)**\n",
    "\n",
    "*samprate*: sample rate of audio file, should be 8000 for current code (code will likely not work with other sample rates!)\n",
    "\n",
    "*chunk_size*: number of seconds to read in each time wav is accessed directly\n",
    "\n",
    "*chunk_pad*: pad chunks of wavs on each end to avoid any issues - 1 sec is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "9hlUy_XuqhSM",
    "outputId": "1e652e45-3c12-488f-b435-ed45ee088d00",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Training new model --------\n",
      "Start time:\n",
      "2018-12-19 16:34:29.727164\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 512, 128, 32) 320         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 512, 128, 32) 0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 256, 64, 32)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 256, 64, 32)  9248        average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 256, 64, 32)  0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 128, 32, 32)  0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 128, 32, 64)  18496       average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 128, 32, 64)  0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 64, 16, 64)   0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 16, 96)   55392       average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 16, 96)   0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 32, 8, 96)    0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 8, 128)   110720      average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 8, 128)   0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 16, 4, 128)   0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 4, 160)   184480      average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 4, 160)   0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 8, 2, 160)    0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 2, 160)    230560      average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 2, 160)    25760       average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 2, 160)    0           conv2d_38[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 2, 160)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 16, 4, 160)   0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 4, 128)   184448      up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 4, 128)   16512       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 4, 128)   0           conv2d_40[0][0]                  \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 4, 128)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 32, 8, 128)   0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 8, 96)    110688      up_sampling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 8, 96)    9312        average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 8, 96)    0           conv2d_42[0][0]                  \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 8, 96)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 64, 16, 96)   0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 64, 16, 64)   55360       up_sampling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 64, 16, 64)   4160        average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 64, 16, 64)   0           conv2d_44[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 64, 16, 64)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 128, 32, 64)  0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 128, 32, 32)  18464       up_sampling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 128, 32, 32)  1056        average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 128, 32, 32)  0           conv2d_46[0][0]                  \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 128, 32, 32)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 256, 64, 32)  0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 256, 64, 32)  9248        up_sampling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 256, 64, 32)  1056        average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 256, 64, 32)  0           conv2d_48[0][0]                  \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 256, 64, 32)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 512, 128, 32) 0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 512, 128, 1)  289         up_sampling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 512, 128, 1)  0           conv2d_50[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,045,569\n",
      "Trainable params: 1,045,569\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_38 to have shape (512, 128, 1) but got array with shape (512, 1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1a739d93a30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m#Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclips_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclips_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'End time:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_38 to have shape (512, 128, 1) but got array with shape (512, 1, 1)"
     ]
    }
   ],
   "source": [
    "#PARAMETERS - Modify parameters before running to change settings!\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#---------TO CHANGE--------:\n",
    "\n",
    "#General\n",
    "use_pretrained_model = False\n",
    "run_model_on_file = False\n",
    "run_model_on_folder = False\n",
    "run_model_on_specified_round = True\n",
    "specified_round = 2\n",
    "specified_model_name = None #defaults to None. use only if use_pretrained_model is True or if you want to specify the name of the model directly\n",
    "run_only_where_ground_truth_available = False #set to True to run only on parts of files where groundtruth labels are available\n",
    "evaluate_detections = False\n",
    "\n",
    "#Model fitting options (use only if use_pretrained_model is False)\n",
    "epochs = 20 #Number of epochs to train for\n",
    "augment = True #whether to augment by overlaying noise (at different levels) on calls\n",
    "conv_dimension = 2\n",
    "\n",
    "#name of audio file (use only if run_model_on_file is True)\n",
    "audio_file_to_predict = 'HM_PET_R11_20170903-20170908_file_2_(2017_09_03-05_44_59)_ASWMUX221163_SS.wav'\n",
    "\n",
    "#Name of audio folder for prediction (use only if run_model_on_folder is True)\n",
    "audio_folder = None\n",
    "\n",
    "#Probability of selecting each call type for training (call types given below). If None, choose calls with probability equal to their occurrence in the training data\n",
    "call_probs = None\n",
    "\n",
    "#---------TO LEAVE ALONE (PROBABLY)--------:\n",
    "\n",
    "#Main directory\n",
    "base_dir = '/home/arianasp/meerkat_detector' #base directory\n",
    "\n",
    "#Subdirectories\n",
    "audio_dir = base_dir + '/data/full_recordings'\n",
    "ground_truth_dir = base_dir + '/ground_truth'\n",
    "model_dir = base_dir + '/models'\n",
    "output_dir = base_dir + '/predictions'\n",
    "code_dir = base_dir + '/dev'\n",
    "clips_dir = base_dir + '/clips'\n",
    "eval_dir = base_dir + '/eval'\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 100\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "call_types = ['cc','sn','ld','mov','agg','alarm','soc','hyb','unk','oth']\n",
    "\n",
    "#Evaluation parameters\n",
    "boundary_thresh = 0.6\n",
    "n_points = 30\n",
    "pckl_paths = [] #NOTE: One can specify pckl_paths, but this is only useful when running evluation ALONE, otherwise anything specified here gets cleared\n",
    "\n",
    "#Other parameters\n",
    "samprate = 8000 \n",
    "chunk_size = 60 \n",
    "chunk_pad = 1 \n",
    "mel = False\n",
    "ml_plan_file = base_dir + '/docs/' + 'audio_labeling_plan_filenames.csv'\n",
    "\n",
    "#SETUP\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#import libraries\n",
    "import sys\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import glob\n",
    "\n",
    "#Set path\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "#Import call detector library\n",
    "from meerkat_call_detector_library import *\n",
    "\n",
    "#NEW MODEL CONSTRUCTION DEFINITIONS (TO MOVE LATER)\n",
    "#inputs: originally spectrogram or output of upper layer\n",
    "#filters: number of filters to use (arbitrary)\n",
    "#n_convs: number of consecutive convolutions to do\n",
    "#output will be time_dim(inputs)/2 x n_filters\n",
    "\n",
    "import keras.layers as layers\n",
    "\n",
    "\n",
    "#Set up file names and paths\n",
    "aug_str = 'noaug'\n",
    "if(augment):\n",
    "    aug_str = 'aug'\n",
    "    \n",
    "dim_str = '_1dconv_'\n",
    "if(conv_dimension==2):\n",
    "    dim_str = '_2dconv_'\n",
    "\n",
    "if(call_probs is not None):\n",
    "    call_probs_str = ''.join([str(s) + '_' for s in call_probs])\n",
    "else:\n",
    "    call_probs_str = 'proportional_'\n",
    "\n",
    "#if model name was specified, use specified name here. otherwise construct name based on parameters.\n",
    "if(specified_model_name is not None):\n",
    "    model_name = specified_model_name\n",
    "else:\n",
    "    model_name = 'cnn_' + str(epochs) + 'epoch_' +  call_probs_str + aug_str + dim_str + time.strftime('%Y%m%d') + '.h5'\n",
    "model_path = model_dir + '/' + model_name\n",
    "\n",
    "#create threshold range for evaluation\n",
    "thresh_range = np.linspace(boundary_thresh+.0001,.9,10)\n",
    "\n",
    "for i in range(2,n_points-9):\n",
    "    thresh_range = np.append(thresh_range,thresh_range[len(thresh_range)-1]+10**(-i)*9)\n",
    "    \n",
    "#TRAIN OR LOAD MODEL\n",
    "#-------------------------------------------------------------------------------\n",
    "        \n",
    "if(use_pretrained_model):\n",
    "\n",
    "    print(\"-------- Loading pretrained model --------\")\n",
    "    print('Model name: ' + model_name)\n",
    "  \n",
    "    #Load pre-trained model\n",
    "    model = load_model(model_dir + '/' + model_name)\n",
    "\n",
    "else:\n",
    "  \n",
    "    print(\"-------- Training new model --------\")\n",
    "    print('Start time:')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    #Construct model\n",
    "    if(conv_dimension==1):\n",
    "        model = construct_unet_model()\n",
    "    else:\n",
    "        model = construct_unet_model_2d()\n",
    "    model.summary()\n",
    "\n",
    "    #Fit model\n",
    "    model.fit_generator(data_generator(clips_dir = clips_dir,batch_size = batch_size, cnn_dim=conv_dimension,mel=mel), epochs=epochs, use_multiprocessing=True, workers=16, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    print('End time:')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    #Save fitted model\n",
    "    print('Saving model as: ' + model_name)\n",
    "    model.save(filepath=model_dir + '/' + model_name) \n",
    "\n",
    "print(\"-------- Done with training or loading model step --------\")\n",
    "\n",
    "\n",
    "#RUN MODEL TO DETECT CALLS\n",
    "#-------------------------------------------------------------------------------\n",
    "#Extract probable calls from wav recording\n",
    "                    \n",
    "if(run_model_on_folder):\n",
    "    \n",
    "    print(\"-------- Running model on folder --------\")\n",
    "    print('Folder path = ' + audio_folder)\n",
    "    \n",
    "    #get all audio files in that folder (or subfolders of it, recursively)\n",
    "    audio_files = glob.glob(audio_folder + '/**/' + '*.wav',recursive=True)\n",
    "    \n",
    "    #print number of files found\n",
    "    print('Found ' + str(len(audio_files)) + ' audio files, running model on all of them')\n",
    "    \n",
    "if(run_model_on_specified_round):\n",
    "    \n",
    "    print(\"-------- Running model on specified round --------\")\n",
    "    print('Round = ' + str(specified_round))\n",
    "\n",
    "    ml_plan = pandas.read_csv(ml_plan_file)\n",
    "    files_to_run = ml_plan[(ml_plan['Round (0-1)'] == str(specified_round)) | (ml_plan['Round (1-2)'] == str(specified_round)) | (ml_plan['Round (2+)'] == str(specified_round))]\n",
    "    files_to_run = files_to_run['Audio filename'].tolist()\n",
    "\n",
    "    audio_files = list()\n",
    "\n",
    "    for f_idx in range(len(files_to_run)):\n",
    "        curr_file = glob.glob(base_dir + '/data/raw_data' + '/**/' + files_to_run[f_idx], recursive=True)[0]\n",
    "        audio_files.append(curr_file)\n",
    "\n",
    "    #print number of files found\n",
    "    print('Found ' + str(len(audio_files)) + ' audio files, running model on all of them')\n",
    "\n",
    "if(run_model_on_folder or run_model_on_specified_round):\n",
    "    \n",
    "    for i in range(len(audio_files)):\n",
    "        \n",
    "        audio_file = audio_files[i]\n",
    "        \n",
    "        print('Running predictions on file: ')\n",
    "        print(audio_file)\n",
    "        \n",
    "        aud = wave.open(audio_file,'rb')\n",
    "        \n",
    "        #time bounds for extraction\n",
    "        if(run_only_where_ground_truth_available):\n",
    "            labels = get_ground_truth_labels(wav_name = os.path.basename(audio_file), ground_truth_dir = ground_truth_dir)\n",
    "            if(labels is None):\n",
    "                print('No ground truth data found - skipping this file')\n",
    "                continue\n",
    "            else:\n",
    "                [t_start, t_end] = get_start_end_time_labels(labels)\n",
    "        else:\n",
    "            t_start = 1\n",
    "            t_end = np.floor(aud.getnframes()/aud.getframerate()/60)*60\n",
    "        \n",
    "        #start at least 1 sec in to avoid problems of wrong input size in next step\n",
    "        if(t_start < 1):\n",
    "            t_start = 1\n",
    "        \n",
    "        #Store parameters in extraction_params object\n",
    "        wav_path = audio_file\n",
    "        audio_name = os.path.basename(audio_file)\n",
    "        pckl_path = output_dir + '/' + audio_name[0:(len(audio_name)-4)] + \"_label_\" + model_name[0:(len(model_name)-3)] + '_' + str(t_start) + '-' + str(t_end) + \".pckl\"\n",
    "        \n",
    "        #Append to list of created pckl paths\n",
    "        pckl_paths.append(pckl_path)\n",
    "        \n",
    "        #if path to extraction results already exists, do not run. otherwise run.\n",
    "        if(not(os.path.exists(pckl_path))):\n",
    "            extraction_params = CallExtractionParams(model_path = model_path, wav_path = wav_path, pckl_path=pckl_path, samprate = samprate, t_start = t_start, t_end = t_end)\n",
    "            \n",
    "            #if SOUNDFOC is in filename, this indicates a different type of sound file - don't run!\n",
    "            if(re.search('SOUNDFOC',audio_file)==None):\n",
    "                extract_scores(model, extraction_params, mel=mel)\n",
    "\n",
    "#Run model on a specific file\n",
    "if(run_model_on_file):\n",
    "    \n",
    "    print('--------Running model on specific file---------')\n",
    "\n",
    "    #create paths to prediction files (wav and pckl)\n",
    "    wav_path = audio_dir + '/' + audio_file_to_predict\n",
    "    \n",
    "    print(wav_path)\n",
    "    \n",
    "    aud = wave.open(wav_path,'rb')\n",
    "    \n",
    "    #tibase_dir = '/home/arianasp/meerkat_detector'me bounds for extraction\n",
    "    if(run_only_where_ground_truth_available): #TODO: add option to find labeled portion and run only for this\n",
    "        labels = get_ground_truth_labels(wav_name = audio_file_to_predict, ground_truth_dir = ground_truth_dir)\n",
    "        if(labels is None):\n",
    "            print('No ground truth data found - set run_only_where_ground_truth_available to False to run on this file')\n",
    "            t_start = None\n",
    "        else:\n",
    "            [t_start, t_end] = get_start_end_time_labels(labels)\n",
    "        #start at least 1 sec in to avoid problems with wrong matrix size in next step\n",
    "        if t_start < 1:\n",
    "            t_start = 1\n",
    "    else:\n",
    "        t_start = 1\n",
    "        t_end = np.floor(aud.getnframes()/aud.getframerate()/60)*60\n",
    "        \n",
    "    if(t_start is not None):\n",
    "    \n",
    "        #create path to output file\n",
    "        pckl_path = output_dir + '/' + audio_file_to_predict[0:(len(audio_file_to_predict)-4)] + \"_label_\" + model_name[0:(len(model_name)-3)] + '_' + str(t_start) + '-' + str(t_end) + \".pckl\"\n",
    "    \n",
    "        pckl_paths = [pckl_path]\n",
    "        extraction_params = CallExtractionParams(model_path = model_path, wav_path = wav_path, pckl_path = pckl_path, samprate = samprate, t_start = t_start, t_end = t_end)\n",
    "        extract_scores(model, extraction_params, mel=mel)\n",
    "                \n",
    "#EVALUATE DETECTIONS\n",
    "#-------------------------------------------------------------------------------\n",
    "if(evaluate_detections & (pckl_paths is not None)):\n",
    "    \n",
    "    #for file_idx in range(len(pckl_files)):\n",
    "    for file_idx in range(len(pckl_paths)):\n",
    "\n",
    "        pckl_path = pckl_paths[file_idx]\n",
    "\n",
    "        run_evaluation(pckl_path = pckl_path,thresh_range=thresh_range,save_dir =eval_dir,ground_truth_dir = ground_truth_dir,call_types = call_types, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "meerkat_call_detector.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
